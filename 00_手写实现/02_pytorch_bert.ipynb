{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# 下面的text 是一个不同样本呢以\\n 分割开的字符串\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")\n",
    "\n",
    "# 去除特殊字符\n",
    "sentences = re.sub('[.,!?\\\\-]','', text.lower()).split('\\n')\n",
    "word_list = list(set(' '.join(sentences).split(' ')))  # ['hello', 'how', 'are', 'you',...]\n",
    "word2idx = {'[PAD]':0, '[CLS]':1, '[SEP]':2, '[MASK]':3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word2idx[w] = i + 4\n",
    "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(arr)  # 获取每个样本的id 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Parameters\n",
    "maxlen = 30        # 一条样本的最大长度设置为30\n",
    "batch_size = 6     #\n",
    "max_pred = 5       # 由于样本长度不长，我们每个句子最多mask 5个token\n",
    "n_layers = 6      \n",
    "n_heads = 12\n",
    "d_model = 768      # embedding 的维度\n",
    "d_ff = 768*4       # 4*d_model, FeedForward dimension  特征提取的维度\n",
    "d_k = d_v = 64     # dimension of K(=Q), V\n",
    "n_segments = 2     # 拼接两条句子为一个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本模式\n",
    "- 在bert 中，两种任务MASK LM 和 NSP 任务是同时进行的\n",
    "    - 每一个样本都是由两条数据拼接而成\n",
    "    - 每一条样本中都随机Mask 掉 15% 的Token\n",
    "    - 同时要保证样本中两条句子相邻：不相邻 = 1：1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据集构造\n",
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # 随机从样本sentences 中取样，拼接， sentences 与 token_list 属于原始样本与 token_id 对应\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        # 拼接两条为一条样本\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM 原文要求15% 但是如果拼接之后的样本很短，15%还不到一个token 会有问题，加上我们自己设置的最多mask 5个\n",
    "        # 需要被mask 的数量\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))  \n",
    "        # 可能被mask 的位置，cls, sep 这些去除掉\n",
    "        candidate_mask_pos = [i for i, token in enumerate(input_ids)\n",
    "                              if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
    "        # mask 位置是随机选择的，可以先mask 再取前n_pred 个\n",
    "        shuffle(candidate_mask_pos)\n",
    "        masked_tokens, masked_pos = [], [] \n",
    "        for pos in candidate_mask_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80% 的直接替换为Mask\n",
    "                input_ids[pos] = word2idx['[MASK]']\n",
    "            elif random() > 0.9:  # 10% 的替换为随机错误的单词，剩下10%的保持不变，可以不进行调整\n",
    "                # 随机找到一个不同的单词，进行替换\n",
    "                index = randint(0, vocab_size-1)\n",
    "                while index < 4:  # 替换后的单词不能是CLS SEP PAD 这种\n",
    "                    index = randint(0, vocab_size-1)\n",
    "                input_ids[pos] = index\n",
    "        \n",
    "        # PAD 填充，固定每个batch 的长度\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)  # 这点不是很理解，segment_id 用于区分不同句子，这样添加不是乱了\n",
    "\n",
    "        # PAD 填充 \n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "    \n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意 bert 中pos_embeded 是要参与到模型更新的，这点与transformer 不同\n",
    "\n",
    "- batch normalization对一批样本同一纬度特征做归一化，一列身高的平均值\n",
    "- 而layer normalization是对单个样本的所有维度特征做归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成self_attn_mask 矩阵\n",
    "- 定义激活函数gelu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding mask \n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    pad_attn_mask = seq_q.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
    "\n",
    "# FFN 层激活函数使用的是gelu \n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入数据并进行编码\n",
    "    - [batch_size, seq_len] --> [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含字编码 + 位置编码 + 分隔句子编码\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)  # 在最后一个维度进行标准化\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] --> [batch_size, seq_len]\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单层Encoder BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 专门计算多头注意力过程中Q,K,V 矩阵乘积  \n",
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q,K,V 形状 [batch_size, n_heads, seq_q/k/v, d_k/d_k/d_v]\n",
    "        attn_mask  [batch_size, n_heads, seq_q, seq_k]\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  # [batch_size, n_heads, seq_q, seq_k]\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)  # [batch_size, n_heads, seq_q, d_v]\n",
    "        return context\n",
    "\n",
    "# 多头自注意力，这里主要生成Q,K,V 矩阵\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)  # d_k == d_q 可以不等于d_v   (len_k == len_v 可以不等于len_q ，在transformer 中有enc_dec_attn 就可以不等于)\n",
    "        self.fc = nn.Linear(n_heads*d_v, d_model)\n",
    "\n",
    "    def forward(self, seq_q, seq_k, seq_v, attn_mask):\n",
    "        \"\"\"\n",
    "        seq_q, seq_k, seq_v：[batch_size, seq_len, d_model]  在bert 中就是编码后的input\n",
    "        \"\"\"\n",
    "        residual, batch_size = seq_q, seq_q.size(0)\n",
    "        # [B,S,D] -proj-> [B,S,D_new]-proj-> [B, S, n_heads, d_k/d_v] -trans-> [B, n_heads, seq_len, d_k/d_v]\n",
    "        # 根据输入的seq_q, k, v获取 Q,K,V 矩阵\n",
    "        Q = self.W_Q(seq_q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]\n",
    "        K = self.W_K(seq_k).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]\n",
    "        V = self.W_V(seq_v).view(batch_size, -1, n_heads, d_v).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  # [batch_size, n_heads, seq_len, seq_len]\n",
    "        # 计算context: [batch_size, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProduct()(Q, K, V, attn_mask)  # [batch_size, n_heads, seq_q, d_v]\n",
    "        # 经过形状变换\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads*d_v)\n",
    "        output = self.fc(context)  # [batch_size, seq_q, d_model]\n",
    "        return nn.LayerNorm(d_model)(output + residual)  #  [batch_size, seq_q, d_model]\n",
    "\n",
    "# 前向全连接层\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x：[batch_size, seq_q, d_model]  经过多头自注意力机制后的输出\n",
    "        \"\"\"\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "# 一个encoder_block  主要包含多头自注意力 + 前向全连接\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \"\"\"\n",
    "        enc_inputs：经过编码之后的输入\n",
    "        \"\"\"\n",
    "        # 经过多头自注意力之后的输出  [batch_size, seq_q, d_model]\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs  #[batch_size, seq_q, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)  # BERT中的NSP任务，判断前后是否为相邻句子\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        embed_weight = self.embedding.tok_embed.weight  # fc2 和 embedding layer 共享权重\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        # 传入mask_pos 原因是，计算损失时，MLM 任务只需要计算mask位置的损失\n",
    "        output = self.embedding(input_ids, segment_ids)  # [batch_size, seq_q, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)  # [batch_szie, maxlen, maxlen]\n",
    "        for layer in self.layers:\n",
    "            # [batch_size, max_len, d_model]\n",
    "            output = layer(output, enc_self_attn_mask)\n",
    "        # ①NSP 损失准备\n",
    "        # 拿出cls ，经过一层特征提取，用来后续形状变换后，进行二分类\n",
    "        h_pooled = self.fc(output[:, 0])  # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled)  # [batch_size, 2]\n",
    "        # ②MLM损失准备\n",
    "        masked_pos = masked_pos[:,:,None].expand(-1, -1, d_model)  # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos)             # [batch_size, max_pred, d_model]\n",
    "        h_masked = self.activ2(self.linear(h_masked))              # [batch_size, max_pred, d_model]\n",
    "        logits_lm = self.fc2(h_masked)                             # [batch_size, max_pred, vocab_size]\n",
    "        return logits_lm, logits_clsf\n",
    "\n",
    "model = Bert()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 4.590352\n",
      "Epoch: 0002 loss = 2.455920\n",
      "Epoch: 0003 loss = 1.908595\n",
      "Epoch: 0004 loss = 1.635944\n",
      "Epoch: 0005 loss = 1.392686\n",
      "Epoch: 0006 loss = 1.201845\n",
      "Epoch: 0007 loss = 1.032200\n",
      "Epoch: 0008 loss = 0.921432\n",
      "Epoch: 0009 loss = 0.899910\n",
      "Epoch: 0010 loss = 0.831523\n",
      "Epoch: 0011 loss = 0.783036\n",
      "Epoch: 0012 loss = 0.821681\n",
      "Epoch: 0013 loss = 0.794956\n",
      "Epoch: 0014 loss = 0.741466\n",
      "Epoch: 0015 loss = 0.794941\n",
      "Epoch: 0016 loss = 0.767450\n",
      "Epoch: 0017 loss = 0.725908\n",
      "Epoch: 0018 loss = 0.787155\n",
      "Epoch: 0019 loss = 0.759007\n",
      "Epoch: 0020 loss = 0.695979\n",
      "Epoch: 0021 loss = 0.732249\n",
      "Epoch: 0022 loss = 0.754515\n",
      "Epoch: 0023 loss = 0.742332\n",
      "Epoch: 0024 loss = 0.738854\n",
      "Epoch: 0025 loss = 0.697506\n",
      "Epoch: 0026 loss = 0.713531\n",
      "Epoch: 0027 loss = 0.726564\n",
      "Epoch: 0028 loss = 0.692116\n",
      "Epoch: 0029 loss = 0.718726\n",
      "Epoch: 0030 loss = 0.723593\n",
      "Epoch: 0031 loss = 0.703812\n",
      "Epoch: 0032 loss = 0.749539\n",
      "Epoch: 0033 loss = 0.709716\n",
      "Epoch: 0034 loss = 0.691878\n",
      "Epoch: 0035 loss = 0.682202\n",
      "Epoch: 0036 loss = 0.726194\n",
      "Epoch: 0037 loss = 0.696739\n",
      "Epoch: 0038 loss = 0.699379\n",
      "Epoch: 0039 loss = 0.694548\n",
      "Epoch: 0040 loss = 0.688108\n",
      "Epoch: 0041 loss = 0.701974\n",
      "Epoch: 0042 loss = 0.675685\n",
      "Epoch: 0043 loss = 0.684995\n",
      "Epoch: 0044 loss = 0.670491\n",
      "Epoch: 0045 loss = 0.678882\n",
      "Epoch: 0046 loss = 0.670352\n",
      "Epoch: 0047 loss = 0.689254\n",
      "Epoch: 0048 loss = 0.713123\n",
      "Epoch: 0049 loss = 0.691186\n",
      "Epoch: 0050 loss = 0.679567\n"
     ]
    }
   ],
   "source": [
    "### 模型训练\n",
    "for epoch in range(50):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "        # 计算MLM 损失\n",
    "        loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1))\n",
    "        # 计算NSP损失\n",
    "        loss_clsf = criterion(logits_clsf, isNext)\n",
    "        loss = loss_lm + loss_clsf\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thank you Romeo\n",
      "Where are you going today?\n",
      "I am going shopping. What about you?\n",
      "I am going to visit my grandmother. she is not very well\n",
      "================================\n",
      "['[CLS]', 'oh', '[MASK]', 'juliet', '[SEP]', 'oh', 'congratulations', 'juliet', '[SEP]']\n",
      "================================\n",
      "masked tokens list :  [6]\n",
      "predict masked tokens list :  [6]\n",
      "================================\n",
      "0\n",
      "isNext :  False\n",
      "predict isNext :  False\n"
     ]
    }
   ],
   "source": [
    "# 随便拿条数据，验证模型\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
    "print(text)\n",
    "print('================================')\n",
    "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
    "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
    "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "# MLM 位置预测\n",
    "logits_lm = logits_lm.max(2)[1][0].numpy()\n",
    "print('================================')\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "# NSP预测\n",
    "print('================================')\n",
    "logits_clsf = logits_clsf.max(-1)[1].numpy()[0]\n",
    "print(logits_clsf)\n",
    "print('isNext : ', isNext)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
